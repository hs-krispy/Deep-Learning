## 딥러닝

### 데이터 확장

**정확도를 높이기 위한** 기법으로 **입력 이미지(훈련 이미지)를 알고리즘을 동원해 인위적으로 확장해 딥러닝의 인식 수준을 개선**

 **EX) 입력 이미지를 회전, 세로로 이동, crop, flip 등**

<img src="https://user-images.githubusercontent.com/58063806/90220124-880d4400-de42-11ea-8952-22e19278c6bc.PNG" width=70% />

### 층을 깊게 하는 이유

**층을 깊게 한 신경망**은 깊지 않은 신경망과 비교해서 **더 적은 매개변수로 같거나 혹은 그 이상 수준의 표현력을 달성(층의 깊이에 비례해 정확도가 좋아짐)**

<img src="https://user-images.githubusercontent.com/58063806/90221110-847abc80-de44-11ea-860c-dad9b1a5d276.PNG" width=70% />

위의 결과를 보면 **5 x 5 합성곱 연산 1회를 수행하는 것은 매개변수의 수가 5 x 5 = 25개**인 반면, **3 x 3 합성곱 연산을 2회 수행하는 것은 매개변수의 수가 2 x 3 x 3 = 18개**로 더 적은 것을 알 수 있음

- **작은 필터를 겹쳐 신경망을 깊게** 하면 **매개변수의 수를 줄여 넓은 수용 영역(뉴런에 변화를 일으키는 국소적인 공간 영역)을 소화**
- **층을 거듭하면서 ReLU 등의 활성화 함수를 합성곱 계층 사이에 끼움**으로써 **신경망의 표현력을 개선**시킴(**활성화 함수가 신경망에 비선형의 힘을 가하고 비선형 함수가 겹치면서 더 복잡한 것도 표현** 가능)

- 학습해야 할 문제를 **계층적으로 분해**(학습해야 할 문제를 **더 단순한 문제로 대체해 효율적인 학습이 가능하도록** 함)
- 층이 깊어도 제대로 학습할 수 있도록 해주는 **새로운 기술과 환경(빅데이터, 컴퓨터의 연산 능력 등)이 뒷받침**되어야 함

### VGG

합성곱 계층과 풀링 계층으로 구성되는 기본적인 CNN으로 **비중 있는 층(합성곱 계층, 완전연결 계층)을 모두 16층(or 19층)으로 심화**한 것이 특징

<img src="https://user-images.githubusercontent.com/58063806/90225046-a1ff5480-de4b-11ea-924f-2d9f1495908d.PNG" width=70% />

- **3 x 3의 작은 필터를 사용한 합성곱 계층을 연속적으로 거침**

- **합성곱 계층을 2 ~ 4회 연속으로 풀링 계층을 두어 크기를 절반**으로 줄이는 처리를 반복

### GoogLeNet

**인셉션 구조**

<img src="https://user-images.githubusercontent.com/58063806/90225407-46819680-de4c-11ea-869a-32a6a387ef08.PNG" width=70% />

- **세로 방향 깊이뿐 아니라 가로 방향도 깊음**
- **크기가 다른 필터(와 풀링)을 여러 개 적용**하고 그 **결과를 결합**
- **1 x 1 크기의 필터를 사용한 합성곱 계층을 많은 곳에서 사용(채널 쪽으로 크기를 줄이는 것으로 매개변수 제거와 고속 처리에 기여)**

### ResNet

**스킵 연결 - 입력 데이터를 합성곱 계층을 건너뛰어 출력에 바로 더하는 구조**

<img src="https://user-images.githubusercontent.com/58063806/90225823-f951f480-de4c-11ea-9649-ec7c1f2abfdb.PNG" width=60% />

weight layer - 합성곱 계층

입력 x를 연속한 두 합성곱 계층을 건너뛰어 출력에 바로 연결

**단축 경로가 없었다면 두 합성곱 계층의 출력이 F(x)**가 되나, **스킵 연결로 인해 F(x) + x**가 됨

- **역전파 때 신호 감쇠를 막아주는 스킵 연결**로 인해 **층이 깊어져도 효율적인 학습**이 가능

- 입력 데이터를 그대로 전달하므로 **역전파 때도 상류의 기울기를 그대로 하류로** 보냄
- **앞 층에 의미 있는 기울기 전달**이 가능(**층을 깊게 할수록 기울기가 작아지는 소실 문제를 줄여줌**)



##### 전이 학습 - 학습된 가중치(or 그 일부)를 다른 신경망에 복사한 다음, 그 상태로 재학습을 수행(보유한 데이터셋이 적을때 유용)