## 수치 미분

미분이란 특정 순간의 변화량을 뜻함

<img src="https://user-images.githubusercontent.com/58063806/88197042-e2b8e300-cc7c-11ea-942b-4b7129768705.JPG" width=50% />

x의 작은 변화가 f(x)를 얼마나 변화시키는가

반올림 오차 - 소수점 8자리 이하의 값은 반올림 해서 0.0으로 나타나게 되고 올바른 표현을 할 수 없는 문제

(10^-4 정도의 값을 사용하면 좋은 결과를 얻을 수 있음)

수치 미분 - x위치의 함수의 기울기(접선)에 해당하는 진정한 미분과 달리 x와 (x+h) 사이의 기울기에 해당함(h를 무한히 0으로 좁히는 것이 불가능하기 때문)

<img src="https://user-images.githubusercontent.com/58063806/88197044-e3517980-cc7c-11ea-98d1-899c1ec62866.JPG" width=50% />

**중심(중앙) 차분 - 이러한 오차를 줄이기 위해 (x + h)와 (x - h) 일 때의 함수 f의 차분을 계산하는 방법**

```python
# 아주 작은 차분으로 미분하는 것
def numerical_diff(f, x):
    h = 1e-4
    return (f(x + h) - f(x - h)) / (2 * h)
```

해석적 - 수식을 전개해서 미분하는 것**(오차를 포함하지 않는 진정한 미분 값을 구해줌)**

```python
import numpy as np
import matplotlib.pyplot as plt

def function_1(x):
    return 0.01*x**2 + 0.1*x

def numerical_diff(f, x):
    h = 1e-4
    return (f(x + h) - f(x - h)) / (2 * h)

x = np.arange(0.0, 20.0, 0.1)
y = function_1(x)
plt.xlabel("x")
plt.ylabel("y")
plt.plot(x, y, label="y = 0.01x^2 + 0.1x")
plt.legend()
plt.show()
print(numerical_diff(function_1, 5)) # 0.1999999999990898 출력
print(numerical_diff(function_1, 10)) # 0.2999999999986347 출력
```

<img src="https://user-images.githubusercontent.com/58063806/88197047-e3ea1000-cc7c-11ea-86e3-8aa2fb451d3e.JPG" width=50% />

위의 함수를 해석적으로 미분해서 진정한 미분의 값을 구하면 각각 0.2, 0.3이 나오는데 이는 수치 미분과 거의 차이가 없음

### 편미분

변수가 여럿인 함수에 대한 미분

```python
# f(x0, x1) = x0^2 + x1^2
# x0 = 3, x1 = 4 일 때
def function_tmp1(x0):
    return x0*x0 + 4.0**2.0

def function_tmp2(x1):
    return 3.0**2.0 + x1*x1

print(numerical_diff(function_tmp1, 3.0)) # 6.00000000000378 출력
print(numerical_diff(function_tmp2, 4.0)) # 7.999999999999119 출력
```

변수가 하나인 함수를 정의하고, 그 함수를 미분하는 형태로 구현

해석적으로 미분해서 나오는 진정한 미분의 값 6, 8과 거의 차이가 없음

## 기울기

모든 변수의 편미분을 벡터로 정리한 것

```python
def numerical_gradient(f, x):
    h = 1e-4
    grad = np.zeros_like(x) # x와 형상이 같고 원소가 모두 0인 배열 생성
    for idx in range(x.size):
        tmp_val = x[idx]
        x[idx] = tmp_val + h
        fxh1 = f(x)
        x[idx] = tmp_val - h
        fxh2 = f(x)
        grad[idx] = (fxh1 - fxh2) / (2 * h)
        x[idx] = tmp_val
    return grad
# 기울기
print(numerical_gradient(function_2, np.array([3.0, 4.0]))) # [6. 8.] 출력
print(numerical_gradient(function_2, np.array([0.0, 2.0]))) # [0. 4.] 출력
print(numerical_gradient(function_2, np.array([3.0, 0.0]))) # [6. 0.] 출력
```

<img src="https://user-images.githubusercontent.com/58063806/88197054-e3ea1000-cc7c-11ea-99bf-c82d9ca39a3f.JPG" width=50%/>

**기울기가 가리키는 쪽은 각 장소에서 함수의 출력을 가장 크게 줄이는 방향** 

### 경사법

신경망에서 학습시에 최적의(손실 함수가 최솟값이 될 때) 매개변수를 찾아야하는데 이때 기울기를 잘 이용해 함수의 최솟값을 찾는 것 (**현 위치에서 기울어진 방향으로 일정 거리만큼 이동하는 것을 반복해서 함수의 값을 점차 줄이는 것**)

**기울어진 방향으로 가면 함수의 값을 줄일 수 있음** 그로 인해 **최솟값이 되는 장소를 찾는 문제**에서는 **기울기 정보를 단서로 나아가 방향을 설정** 

경사 하강법 - 최솟값을 찾는 경우

경사 상승법 - 최댓값을 찾는 경우

함수가 **극솟값(한정된 범위에서 최솟값인 점), 안정점(어느 방향에서는 극댁값이고 다른 방향에서는 극솟값)이 되는 장소에서는 기울기가 0**이므로 경사법으로 **기울기가 0인 장소를 찾아도 그것이 반드시 최솟값이라고 할 수 없음** 

**복잡하고 찌그러진 모양의 함수라면 평평한 곳으로 파고들면서 고원(학습이 진행되지 않는 정체기)에 빠질 수 있음**

<img src="https://user-images.githubusercontent.com/58063806/88199972-7213c580-cc80-11ea-9117-0fa94e49f337.JPG" width=30%/>

η는 갱신하는 양을 나타내며 신경망 학습에서는 **학습률(한 번의 학습으로 얼마만큼 학습해야 할지, 매개변수 값을 얼마나 갱신하느냐를 정하는 것)**이라고 함

**학습률 값은 미리 특정 값으로 정해두어야하는데** 이 **값이 너무 크거나 작으면 좋은 장소를 찾아갈 수 없으므로** 값을 계속 변경하면서 **올바르게 학습하고 있는지를 확인해야함**

```python
# 경사 하강법 구현
def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x
    for i in range(step_num):
        grad = numerical_gradient(f, x)
        x -= lr * grad
    return x
print(gradient_descent(function_2, init_x=np.array([-3.0, 4.0]), lr=0.1, step_num=100))
# [-6.11110793e-10  8.14814391e-10] 출력, 실제 최솟값 (0, 0)과 거의 동일
print(gradient_descent(function_2, init_x=np.array([-3.0, 4.0]), lr=10.0, step_num=100))
# 학습률이 너무 큰 경우, [-2.58983747e+13 -1.29524862e+12] 출력 (너무 큰 값)
print(gradient_descent(function_2, init_x=np.array([-3.0, 4.0]), lr=1e-10, step_num=100))
# 학습률이 너무 작은 경우, [-2.99999994  3.99999992] 출력 (거의 갱신 X)
```

f - 최적화하려는 함수

init_x - 초기값

lr(learning rate) - 학습률

step_num - 경사법에 따른 반복 횟수

위의 결과를 보면 적절한 학습률을 설정하는 것이 중요함을 알 수 있음

**하이퍼파라미터** - **가중치와 편향같은 신경망의 매개변수(훈련 데이터와 학습 알고리즘에 의해 자동으로 획득되는)**와는 성질이 다른 **학습률**과 같은 매개변수로 **사람이 직접 설정해야하는 매개변수**

### 신경망에서의 기울기

```python
import sys, os
sys.path.append("deep-learning-from-scratch")
import numpy as np
from common.functions import softmax, cross_entropy_error
from common.gradient import numerical_gradient

class simpleNet:
    def __init__(self):
        self.W = np.random.randn(2, 3) # 정규분포로 초기화

    def predict(self, x):
        return np.dot(x, self.W)

    def loss(self, x, t): # x는 입력 데이터, t는 정답 레이블
        z = self.predict(x)
        y = softmax(z)
        loss = cross_entropy_error(y, t)
        return loss

# def f(W):
#	return net.loss(x, t)
# 간단한 함수는 lambda를 이용하면 더 편리하게 구현  
f = lambda w: net.loss(x, t)

net = simpleNet()
print(net.W) # 가중치 매개변수
x = np.array([0.6, 0.9])
p = net.predict(x)
print(p) # 예측을 수행
print(np.argmax(p)) # 최댓값의 인덱스 
t = np.array([0, 0, 1])
print(net.loss(x, t)) # 손실 함수의 값
dW = numerical_gradient(f, net.W)
print(dW) # 각 원소에 대한 기울기
```

**실행결과**

<img src="https://user-images.githubusercontent.com/58063806/88205141-4ea04900-cc87-11ea-8df2-b41eec48a740.JPG" width=50% />