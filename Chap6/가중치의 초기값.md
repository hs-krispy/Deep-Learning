## 가중치의 초기값

가중치의 초기값을 무엇으로 설정하느냐가 신경망 학습의 성패를 가를 정도로 중요

**초깃값을 0으로 설정하면(가중치를 균일한 값으로 설정) 오차역전파법에서 모든 가중치의 값이 똑같이 갱신**되기 때문에 가중치가 고르게 되어버리고 **가중치를 여러 개 갖는 의미를 사라지게 함**

그러므로 **초기값을 무작위로 설정**해야 함

### 은닉층의 활성화값 분포

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

input_data = np.random.randn(1000, 100)  # 1000개의 데이터
node_num = 100  # 각 은닉층의 노드(뉴런) 수
hidden_layer_size = 5  # 은닉층이 5개
activations = {}  # 이곳에 활성화 결과를 저장

for i in range(hidden_layer_size):
    if i != 0:
        x = activations[i-1]
        
	w = np.random.randn(node_num, node_num) * 1
    # 표준편차가 1인 정규분포
    # w = np.random.randn(node_num, node_num) * 0.01
    # 표준편차가 0.01인 정규분포
    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)
    # Xavier 초기값
    # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)
    # He 초기값
    a = np.dot(x, w)
    z = sigmoid(a)
    activations[i] = z # 활성화 결과 저장
    
for i, a in activations.items():
    plt.subplot(1, len(activations), i+1)
    plt.title(str(i+1) + "-layer")
    plt.hist(a.flatten(), 30, range=(0,1))
plt.show()
```

#### 가중치로 표준편차가 1인 정규분포

<img src="https://user-images.githubusercontent.com/58063806/89256853-f6dae800-d65f-11ea-8de5-2211099074a3.PNG" width=50% />

시그모이드 함수는 출력이 0또는 1에 가까워지면 미분은 0에 다가감

**기울기 소실 - 활성화 값들이 0과 1에 치우쳐 분포**되어있는데 **역전파의 기울기 값이 점점 작아지다가 사라짐** 

#### 가중치로 표준편차가 0.01인 정규분포

<img src="https://user-images.githubusercontent.com/58063806/89258988-4d4a2580-d664-11ea-9ca1-4ccc04033212.PNG" width=50% />

활성화 값들이 0.5 부근에 집중되어서 기울기 소실 문제는 일어나지 않지만 **활성화 값들이 치우쳐있음**

다수의 뉴런이 거의 같은 값을 출력하기 때문에 **뉴런을 여러 개 둔 의미가 없어짐(표현력이 제한)**



**층과 층 사이에 적당하게 다양한 데이터가 흐르게 해야 신경망 학습이 효율적으로 이루어지기 때문에 각 층의 활성화값은 고루 분포되어야 함**

#### Xavier 초기값

일반적인 딥러닝 프레임워크들이 표준적으로 사용하는 초기값

각 층의 활성화값들을 광범위하게 분포시키기 위해 **앞 계층의 노드가 n개면 표준편차가 1/루트n인 분포** 사용

**활성화 함수로 sigmoid 함수 사용**

<img src="https://user-images.githubusercontent.com/58063806/89259552-80d97f80-d665-11ea-9a3f-56708ff10699.PNG" width=50% />

층이 깊어지면서 형태가 다소 일그러지긴 하지만 확실히 앞의 방식보다는 활성화값이 넓게 분포됨

오른쪽으로 일그러지는 것은 **sigmoid함수 대신 tanh 함수(쌍곡선 함수)를 이용하면 개선**

**sigmoid는 (x, y) = (0, 0.5)에서 대칭, tanh는 (x, y) = (0, 0)에서 대칭**

**활성화 함수로 tanh 함수 사용**

<img src="https://user-images.githubusercontent.com/58063806/89260415-32c57b80-d667-11ea-9982-81ff53a88578.PNG" width=50% />

**활성화 함수용으로는 원점 대칭인 함수가 바람직함**

#### He 초기값

**활성화 함수로 ReLU를 이용할때 특화된 초기값**으로 **음의 영역이 0인 ReLU의 특징**때문에 **더 넓게 분포시키기 위해 2배의 계수**가 필요

**앞 계층의 노드가 n개면 표준편차가 2/루트n인 분포** 사용

<img src="https://user-images.githubusercontent.com/58063806/89260418-33f6a880-d667-11ea-9879-22ebce4cbf25.PNG" width=50% />

모든 층에서 균일하게 분포되는 것을 알 수 있음



위의 결과들을 보면 **활성화 함수로 ReLU를 사용할 때는 He 초기값**, **sigmoid나 tanh 등의 S자 모양 곡선일 때는 Xavier 초기값을 사용**하는 것이 현재의 모범사례라고 할 수 있음

### 손글씨 숫자 인식데이터에서 가중치 초깃값 비교

층별 뉴런 수가 100개인 5층 신경망에서 활성화 함수로 ReLU를 사용

<img src="https://user-images.githubusercontent.com/58063806/89261480-1a566080-d669-11ea-96cb-e06bc261344b.PNG" width=55% />

**std = 0.01일 때의 각 층의 활성화값들은 아주 작은 값**들로 이때는 **학습이 거의 이뤄지지 않음**

학습진도는 **He 초기값이 가장 빠른 것**을 알 수 있음