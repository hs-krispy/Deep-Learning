## 매개변수 갱신

### 비등방성 함수

**특정한 지점에서 기울기가 가리키는 지점이 변하는 경우가 존재, 기울기가 가르키는 지점이 하나가 아니라 여러가지**

EX) f(x, y) = 1/20x^2 + y^2

<img src="https://user-images.githubusercontent.com/58063806/89200788-7c708080-d5eb-11ea-8341-27ed542393d4.JPG" width=50% />

**기울기 대부분이 최솟값이 되는 지점 (0, 0)을 가리키지 않음**

### 확률적 경사 하강법(SGD)

최적화 - 매개변수의 최적값을 찾는 문제

**기울어진 방향으로 매개변수를 갱신하는 일을 반복(기울어진 방향으로 일정 거리만 가겠다는 방법)**

```python
class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr
       
    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]
```

**가중치 매개변수 갱신 = 가중치 매개변수 - 학습률 x 가중치 매개변수에 대한 손실 함수의 기울기**

#### SGD의 단점

**비등방성 함수에서는 탐색 경로가 비효율적**

<img src="https://user-images.githubusercontent.com/58063806/89200791-7d091700-d5eb-11ea-97fb-da891296b5a8.JPG" width=55% />

최솟값인 (0, 0)까지 **지그재그의 형태로 이동하므로 비효율적**

### 모멘텀

<img src="https://user-images.githubusercontent.com/58063806/89243366-b9b22e00-d63e-11ea-9844-697daac66219.PNG" width=35% />

v(velocity) - 속도

a - hyper parameter로 0.9 등 1 이하의 값을 취함

av - 믈체가 아무런 힘을 받지 않을 때 서서히 하강시키는 역할 

```python
class Momentum:
    
    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None
        
    def update(self, params, grads):
        if self.v is None:
            self.v = {}
            for key, val in params.items():                                
                self.v[key] = np.zeros_like(val) 
                # val과 같은 크기의 0으로 만들어진 배열을 v[key]에 넣어줌
                
        for key in params.keys():
            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] 
            params[key] += self.v[key]
```

<img src="https://user-images.githubusercontent.com/58063806/89244191-d6e7fc00-d640-11ea-960a-0992f24f0920.PNG" width=55%/>

x축의 힘은 아주 작지만 **방향은 변하지 않아서 한 방향으로 일정하게 가속**

y축의 힘은 크지만 **위아래로 번갈아 받아서 상충하여 y축 방향의 속도는 안정적이지 않음**

**SGD와 비교했을때 x축 방향으로 빠르게 다가가 지그재그 움직임이 줄어듬**

### AdaGrad

학습률이 너무 작으면 학습 시간이 너무 길어지고, 너무 크면 발산해서 학습이 제대로 이루어지지 않음

**학습률 감소 - 학습을 진행하면서 학습률을 점차 줄여가는 방법(매개변수 전체의 학습률 값을 일괄적으로 낮춤)**

AdaGrad는 **개별 매개변수에 적응적으로 학습률을 조정**하면서 학습 진행

<img src="https://user-images.githubusercontent.com/58063806/89245624-2ed43200-d644-11ea-81a3-c9f9c464a108.PNG" width=40% />

⊙ - 행렬의 원소별 곱셈

h - 기존 기울기 값을 제곱하여 계속 더해줌

매개변수를 갱신할 때 **1/루트h를 곱해 학습률을 조정**

매개변수의 원소 중에서 **많이 움직인(크게 갱신된) 원소는 학습률이 낮아짐(학습률의 감소가 매개변수의 원소마다 다르게 적용)**

```python
class AdaGrad:

    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None
        
    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
            
        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
            # 1e-7은 0으로 나누는 경우를 막아줌
```

<img src="https://user-images.githubusercontent.com/58063806/89246256-b0788f80-d645-11ea-82b4-a93a1fc735f5.PNG" width=60% />

**y축 방향**은 **기울기가 커서 처음에는 크게 움직**이지만, 그에 **비례해서 갱신 정도도 큰 폭으로 작아지도록 조정**

### Adam

모멘텀과 AdaGrad를 융합한 듯한 방법

학습률, 일차 모멘텀용 계수(기본설정값 : 0.9), 이차 모멘텀용 계수(기본설정값 : 0.999) 세 가지 하이퍼 파라미터를 설정

<img src="https://user-images.githubusercontent.com/58063806/89253966-01de4a00-d659-11ea-9187-9156c64b2f59.PNG" width=60% /> 

**모멘텀과 비슷하게 공이 그릇 바닥을 구르듯** 움직이지만 **학습의 갱신 강도를 적응적으로 조정하기 때문에 공의 좌우 흔들림이 적음**



풀어야 할 문제에 따라 좋은 갱신방법이 다르고 하이퍼 파라미터를 어떻게 설정하느냐에 따라서도 결과가 달라지므로 **각 문제에 맞는 갱신방법을 선택하는 것이 중요(요즘은 Adam을 많이 사용)** 

### 각 기법을 손글씨 숫자 인식에 적용한 결과

각 층이 100개의 뉴런으로 구성된 5층 신경망에서 ReLU를 활성화 함수로 사용

<img src="https://user-images.githubusercontent.com/58063806/89254702-bcbb1780-d65a-11ea-8088-1834f169e79b.PNG" width=60% />

위의 결과를 보면 SGD의 학습 진도가 가장 느리고 AdaGrad가 가장 빠른 것을 알 수 있음

**일반적으로 SGD보다 다른 세 기법이 빠르게 학습하고, 최종 정확도도 높게 나타남**