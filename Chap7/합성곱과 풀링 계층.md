## CNN의 전체 구조

완전 연결 - 인접하는 계층의 모든 뉴런과 결합

Affine 계층 - 완전히 연결된 계층

**완전연결 계층(Affine 계층)으로 이뤄진 네트워크**

<img src="https://user-images.githubusercontent.com/58063806/89733238-29854600-da8f-11ea-9818-a6599ecd0202.JPG" width=80% />

완전연결 신경망은 **Affine 계층 뒤에 활성화 함수를 갖는 ReLU(또는 Sigmoid) 계층이 이어지는 조합이 4개**가 쌓였고, 마지막 **5번째 층은 Affine 계층 뒤에 소프트맥스 계층에서 최종 결과를 출력**

**CNN으로 이뤄진 네트워크**

<img src="https://user-images.githubusercontent.com/58063806/89733239-2be7a000-da8f-11ea-897e-025611d12965.JPG" width=80%/>

위의 그림을 보면 합성곱(Conv)과 폴링(Pooling) 계층이 추가된 것을 알 수 있음

**Affine-ReLU 연결이 Conv-ReLU-(Pooling)으로 바뀜**(Pooling은 생략하기도 함)

**출력에 가까운 층에서는 Affine-ReLU 구성 사용가능**하며 **마지막 출력 계층에서는 Affine-Softmax 연결을 그대로 사용**

## 합성곱 계층

CNN에서 계층 사이에는 **3차원 데이터같이 입체적인 데이터가 흐른다는 점**에서 완전연결 신경망과 다름 

#### 완전연결 계층의 문제점

**데이터의 형상이 무시**됨

EX) **데이터가 이미지**인 경우는 **가로, 세로, 채널(색상)으로 구성된 3차원 데이터**이지만 **완전연결 계층에 입력할 때는 3차원 데이터를 평평한 1차원 데이터(784개)로 평탄화**해줘야 함

3차원 속에서 **의미를 갖는 본질적인 패턴(공간적으로 가까운 픽셀은 값이 비슷, RGB의 각 채널은 서로 밀접하게 관련 등)을 무시하고 모든 입력 데이터를 동등한 뉴런(같은 차원의 뉴런)으로 취급**해서 **형상에 담긴 정보를 살릴 수 없음**

하지만 합성곱 계층은 형상을 유지해서 형상을 가진 데이터를 제대로 이해할(가능성이 있는)수 있음

**입력 특징 맵 - 합성곱 계층의 입력 데이터**

**출력 특징 맵 - 합성곱 계층의 출력 데이터**

#### 합성곱 연산(필터 연산)

필터의 **윈도우(그림의 3 x 3 회색부분)를 일정 간격으로 이동해가며** 입력 데이터에 적용

<img src="https://user-images.githubusercontent.com/58063806/89752953-48c4b780-db11-11ea-9a4c-60284ffee3af.PNG" width=60% />

**입력과 필터에서 대응하는 원소끼리 곱한 총합을(단일 곱셈-누산)** 출력의 해당 장소에 저장

CNN에서는 **필터의 매개변수가 그동안의 가중치에 해당**

**편향(1 x 1)은 필터를 적용한 후에 데이터에 각각 더해짐**

#### 패딩

합성곱 연산을 몇 번이나 되풀이하는 **심층 신경망**에서는 **합성곱 연산을 거칠 때마다 출력의 크기가 작아지면 어느 시점에서는 1이 되어버리므로 더 이상 합성곱 연산이 불가**

**이를 해결하기 위해 합성곱 연산을 수행하기 전에 입력 데이터 주변을 특정 값으로 채움(출력 크기를 조정할 목적)**

<img src="https://user-images.githubusercontent.com/58063806/89753122-02bc2380-db12-11ea-9e7d-a93d416fe293.PNG" widht=100% />

위의 그림을 보면 패딩을 1 추가해서 입력의 크기가 (6, 6)이 됨 (2 추가하면 (8, 8), 3 추가하면 (10, 10))

#### 스트라이드

필터를 적용하는 위치의 간격

<img src="https://user-images.githubusercontent.com/58063806/89753491-7f033680-db13-11ea-85f4-f38667b27fff.PNG" width=60% />

위의 그림을 보면 필터를 적용하는 윈도우가 2칸씩 이동

<img src="https://user-images.githubusercontent.com/58063806/89753812-a3134780-db14-11ea-8f80-d59857c806f2.PNG" width=40% />

**H, W - 입력의 크기	FH, HW - 필터 크기	OH, OW - 출력 크기	P - 패딩	S - 스트라이드**

단, 위의 식이 **정수로 나눠떨어지는 값이어야 한다는 점에 주의해야 함**

#### 3차원 데이터의 합성곱

채널 방향으로 특징 맵이 늘어남

**입력 데이터와 필터의 합성곱 연산을 채널마다 수행**하고 그 값들을 더해서 하나의 출력을 얻음

<img src="https://user-images.githubusercontent.com/58063806/89754359-85df7880-db16-11ea-9a33-b670798d633b.PNG" width=60%/>

**입력 데이터의 채널 수와 필터의 채널 수가 같아야함**

다수의 필터를 사용하면 출력으로 다수의 채널을 내보낼 수 있음

 <img src="https://user-images.githubusercontent.com/58063806/89782406-3158ef00-db50-11ea-82a3-12995a23b86b.JPG" width=60% />

**필터의 가중치 데이터는 4차원 데이터(출력 채널 수, 입력 채널 수, 높이, 너비)**

편향은 **각 채널당 하나의 값**으로 구성됨 

#### 배치 처리

<img src="https://user-images.githubusercontent.com/58063806/89783205-82b5ae00-db51-11ea-8c53-ec8f3b0c826e.JPG" width=70% />

각 계층을 흐르는 데이터의 차원을 하나 늘려 4차원 데이터로 저장(데이터 수, 채널 수, 높이, 너비)

신경망에 **4차원 데이터가 하나 흐를 때마다 데이터 N개에 대한 합성곱 연산**이 이뤄짐**(N회 분 처리를 한 번에 수행)**

## 풀링 계층

세로, 가로 방향의 공간을 줄이는 연산

- 합성곱 계층과 달리 **학습해야 할 매개변수가 없음**
- **입력 데이터의 채널 수 그대로 출력 데이터로 내보냄**(채널 수를 바꾸지 않음)
- **입력 데이터의 변화에 영향을 적게 받음**

<img src="https://user-images.githubusercontent.com/58063806/89783616-3323b200-db52-11ea-8056-932594034005.JPG" width=60% />

위는 **2 x 2 최대 풀링(이미지 인식 분야에서 주로 사용)을 스트라이드 2**로 처리하는 것으로 2 x 2 영역에서 가장 큰 원소 하나를 꺼냄

**(풀링의 윈도우 크기와 스트라이드는 같은 값으로 설정하는 것이 일반적)**

