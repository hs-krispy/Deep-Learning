## 신경망

신경망은 **입력층, 은닉층(사람 눈에 보이지 않음), 출력층 3층으로 구성**되지만 **가중치를 갖는 층은 2개**뿐이기 때문에 **2층 신경망**이라고함

### 활성화 함수

**입력 신호의 총합을 출력 신호로 변환하는 함수(입력 신호의 총합이 활성화를 일으키는지 정함)**

**계단 함수** **- 임계값을 경계로 출력이 바뀌는 함수로 퍼셉트론에서는 활성화 함수로 계단 함수를 이용함**

**시그모이드 함수 - 신경망에서 자주 이용하는 활성화 함수(S자 모양의 형태를 가짐)**

h(x) = 1/(1+e^(-x) )

**퍼셉트론과 신경망의 주된 차이는 활성화 함수**라고 할 수 있음

```python
# 계단 함수와 시그모이드 함수 구현
import numpy as np
import matplotlib.pyplot as plt
def step_fuction(x):
    y = x > 0
    return y.astype(np.int)
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
y2 = step_fuction(x)
plt.plot(x, y, label="시그모이드 함수")
plt.plot(x, y2, "--", label="계단 함수")
plt.rc("font", family="Malgun Gothic")
plt.legend()
plt.show()
```

<img src="https://user-images.githubusercontent.com/58063806/87908457-5dc5a200-caa1-11ea-9882-9c57849603ee.PNG" width=55%/>

위의 그래프를 보면 **계단 함수는 0을 경계로 출력이 갑자기 바뀌는** 반면 **시그모이드 함수는 부드러운 곡선의 형태로 입력에 따라 출력이 연속적으로 변하는 것**을 볼 수 있음

하지만 **입력이 아무리 작거나 커도 출력은 0에서 1 사이**라는 공통점을 가짐 

### 비선형 함수

선형 함수 - 출력이 입력의 상수배만큼 변하는 함수

선형이 아닌, **직선 1개로는 그릴 수 없는 함수**로 **신경망에서는 활성화 함수로 비선형 함수를 사용**

예를 들어 선형 함수를 사용하면 **(3층 네트워크 -> y(x) = h(h(h(x))) = c^3x) 과 같이 은닉층이 없는 네트워크로 표현이 가능**하고 **여러 층으로 구성하는 이점을 살릴 수 없으며 층을 깊게하는 의미가 없어짐**

ReLU 함수 - 최근에 신경망 분야에서 주로 이용하는 활성화 함수

x > 0 이면 x (입력 그대로 출력)

x <= 0 이면 0

```python
# ReLU 함수 구현
import numpy as np
def relu(x):
    return np.max(0, x)
```

### 3층 신경망 구성

```python
import numpy as np
def identity_function(x):
    return x
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
# 가중치와 편향을 초기화
def init_network():
    network = {}
    network["W1"] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
    network["b1"] = np.array([0.1, 0.2, 0.3])
    network["W2"] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
    network["b2"] = np.array([0.1, 0.2])
    network["W3"] = np.array([[0.1, 0.3], [0.2, 0.4]])
    network["b3"] = np.array([0.1, 0.2])
    return network
# 입력 신호를 출력으로 변환, np.dot()을 이용해서 행렬을 곱함
def forward(network, x):
    W1, W2, W3 = network["W1"], network["W2"], network["W3"]
    b1, b2, b3 = network["b1"], network["b2"], network["b3"]
    a1 = np.dot(x, W1) + b1  # 입력층에서 1층으로
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2  # 1층에서 2층으로
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3  # 2층에서 출력층으로
    y = identity_function(a3)
    return y
network = init_network()
x = np.array([1.0, 0.5])
y = forward(network, x)
print(y) # [0.31682708 0.69627909] 출력
```

기계학습 문제는 분류와 회귀로 나뉨

**분류 - 데이터가 어느 클래스에 속하는 가**

**EX) 사진 속 인물의 성별을 분류**

**회귀 - 입력 데이터에서 (연속적인) 수치를 예측하는 문제**

**EX) 사진 속 인물의 몸무게를 예측** 

출력층의 활성화 함수는 풀고자 하는 문제의 성질에 맞게 정함

**회귀 - 항등 함수**

**2클래스 분류 - 시그모이드 함수** 

**다중 클래스 분류 - 소프트맥스 함수**

### 소프트맥스 함수

<img src="https://user-images.githubusercontent.com/58063806/87908455-5d2d0b80-caa1-11ea-9ecb-8f77945cfd2e.png" width=55% />

n - 출력층의 뉴런 수

Yk - k번째 출력

ak - k번째 입력

**소프트맥스의 출력은 모든 입력 신호로부터 영향을 받음**

```python
# 소프트맥스 함수 구현
import numpy as np
def softmax(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
```

하지만 위와 같이 함수를 구현하면 **오버플로 문제**가 발생함

<img src="https://t1.daumcdn.net/cfile/tistory/220C773B593E518207" width=40% />

위와 같은 식으로 개선시킬 수 있는데 **소프트맥스의 지수 함수를 계산할 때 어떤 정수를 더해도 결과는 바뀌지 않는다는 것을 의미**함

**입력 신호 중 최댓값을 이용하는 것이 일반적**

```python
# 개선된 소프트맥스 함수 구현
import numpy as np
def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a - c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
# a = np.array([0.3, 2.9, 4.0])
# y = softmax(a)
# print(y) # [0.01821127 0.24519181 0.73659691] 출력
# print(np.sum(y)) # 1.0 출력
```

**입력 값을 0 ~ 1사이의 값으로 모두 정규화해서 출력, 출력 값들의 총합은 항상 1이 되는 특성을 가짐**

이로 인해 소프트맥스 함수의 출력은 **확률**로 해석가능

신경망을 이용한 분류에서는 일반적으로 **가장 큰 출력을 내는 뉴런에 해당하는 클래스로만 인식**하는데 소프트맥스 함수를 적용해도 **각 원소의 대소 관계는 변하지 않으므로** **신경망으로 분류할 때는 출력층의 소프트맥스 함수를 생략해도 됨**

기계학습의 문제 풀이는 학습과 추론 두 단계를 거침

**학습 - 모델을 학습**

**추론 - 학습한 모델로 미지의 데이터에 대해서 추론(분류)를 수행**

**추론 단계에서는 추력층의 소프트맥스 함수를 생략하는 것이 일반적**

### 출력층의 뉴런 수 결정

풀려는 문제에 맞게 적절히 정해야 함

EX) 입력 이미지를 숫자 0 ~ 9 중 하나로 분류하고자 하면 출력층의 뉴런을 10개로 설정

**뉴런의 회색 농도가 해당 뉴런의 출력 값의 크기를 의미**

EX) y2 뉴런이 색이 가장 짙으면 가장 큰 값을 출력 -> 신경망은 y2 클래스 선택(이미지를 숫자 2로 판단)