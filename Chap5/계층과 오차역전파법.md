## 계층 구현

#### 곱셈 계층

```python
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None

    def forward(self, x, y):
        self.x = x
        self.y = y                
        out = x * y
        
        return out

    def backward(self, dout):
        dx = dout * self.y  # x와 y를 바꾼다.
        dy = dout * self.x

        return dx, dy
    
apple = 100
apple_num = 2
tax = 1.1

mul_apple_layer = MulLayer()
mul_tax_layer = MulLayer()

# forward
apple_price = mul_apple_layer.forward(apple, apple_num) # 200
price = mul_tax_layer.forward(apple_price, tax) # 220

# backward
dprice = 1
dapple_price, dtax = mul_tax_layer.backward(dprice)
dapple, dapple_num = mul_apple_layer.backward(dapple_price)

print("dApple_price:", dapple_price) # 1.1출력(1.1 x 1)
print("dApple:", dapple) # 2.2출력(1.1 x 2)
print("dApple_num:", int(dapple_num)) # 110출력(1.1 x 100)
print("dTax:", dtax) # 200출력(1 x 200)
```

#### 덧셈 계층

```python
class AddLayer:
    def __init__(self):
        pass

    def forward(self, x, y):
        out = x + y

        return out

    def backward(self, dout):
        dx = dout * 1
        dy = dout * 1

        return dx, dy
```

#### 적용

```python
apple = 100
apple_num = 2
orange = 150
orange_num = 3
tax = 1.1

# layer
mul_apple_layer = MulLayer()
mul_orange_layer = MulLayer()
add_apple_orange_layer = AddLayer()
mul_tax_layer = MulLayer()

# forward
apple_price = mul_apple_layer.forward(apple, apple_num)  # 200
orange_price = mul_orange_layer.forward(orange, orange_num)  # 450
all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # 650
price = mul_tax_layer.forward(all_price, tax)  

# backward
dprice = 1
dall_price, dtax = mul_tax_layer.backward(dprice)  
dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  
dorange, dorange_num = mul_orange_layer.backward(dorange_price)  
dapple, dapple_num = mul_apple_layer.backward(dapple_price)  

print("dAll_price:", dall_price) # 1.1 출력
print("dApple_price:", dapple_price) # 1.1 출력(미분 그대로)
print("dOrange_price:", dorange_price) # 1.1 출력(미분 그대로)
print("price:", int(price)) # 715 출력
print("dApple:", dapple) # 2.2 출력(1.1 x 2)
print("dApple_num:", int(dapple_num)) # 110 출력(1.1 x 100)
print("dOrange:", dorange) # 3.3 출력(1.1 x 3)
print("dOrange_num:", int(dorange_num)) # 165 출력(1.1 x 150)
print("dTax:", dtax) # 650 출력
```

### 활성화 함수 계층

#### ReLU 계층

```python
class Relu:
    def __init__(self):
        self.mask = None 

    def forward(self, x):
        self.mask = (x <= 0)
        out = x.copy()
        out[self.mask] = 0

        return out

    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout

        return dx
```

**mask - True, False로 구성된 넘파이 배열(순전파의 입력 x의 원소 값이 0이하인 인덱스는 True, 나머지는 False로 유지)**

순전파의 **입력이 0보다 크면 역전파는 상류의 값을 그대로**, **0이하면 역전파 때 하류로 신호를 보내지 않음(0을 보냄)**

#### Sigmoid 계층

```python
class Sigmoid:
    def __init__(self):
        self.out = None

    def forward(self, x):
        out = sigmoid(x)
        self.out = out
        return out

    def backward(self, dout):
        dx = dout * (1.0 - self.out) * self.out

        return dx
```

<img src="https://user-images.githubusercontent.com/58063806/89139499-cb7dcd80-d579-11ea-9c87-3728fe4c8df6.PNG" width=60% />

**순전파의 출력 y만으로 역전파 계산 가능**

**out** - 역전파 계산에 사용하기 위한 **순전파의 출력을 저장**

#### Affine, Softmax 계층

**affine - 행렬의 곱** 

<img src="https://user-images.githubusercontent.com/58063806/89140362-64ade380-d57c-11ea-8971-d0bd1666e028.PNG" width=70% />

WT - W의 행과 열이 바뀐 전치행렬 

```python
class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.dW = None
        self.db = None

    def forward(self, x):
        self.x = x
        out = np.dot(self.x, self.W) + self.b

        return out

    def backward(self, dout):
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout) # 가중치의 역전파
        self.db = np.sum(dout, axis=0) # 편향의 역전파
        
        return dx
```

#### Softmax-with-Loss 계층

<img src="https://user-images.githubusercontent.com/58063806/89147121-c415ee80-d590-11ea-89ee-9bbe2ba5a430.PNG" width=60%/>

Softmax 계층은 입력(a1, a2, a3)를 정규화하여 (y1, y2, y3)을 출력

Cross Entropy Error 계층은 softmax 출력과 정답 레이블(t1, t2, t3)을 받고, 손실 L을 출력

**softmax 계층의 역전파는 (y1 - t1, y2- t2, y3 - t3)으로 softmax 계층의 출력과 정답 레이블의 차분(오차)**

```python
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None # 손실함수
        self.y = None    # softmax의 출력
        self.t = None    # 정답 레이블(원-핫 인코딩 형태)
        
    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y, self.t)
        
        return self.loss

    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        dx = (self.y - self.t) / batch_size
        
        return dx
```

## 오차역전파법 구현

```python
        # 계층 생성
        self.layers = OrderedDict()
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
        self.layers['Relu1'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])

        self.lastLayer = SoftmaxWithLoss()
        
    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)
        
        return x
    
     def gradient(self, x, t):
        # forward
        self.loss(x, t)

        # backward
        dout = 1
        dout = self.lastLayer.backward(dout)
        
        layers = list(self.layers.values())
        layers.reverse() # 역순
        for layer in layers:
            dout = layer.backward(dout)
```

OrderedDict - 순서가 있는 딕셔너리(딕셔너리에 추가한 순서를 기억)

순전파 때는 추가한 순서대로 forward() 호출(역전파 때는 계층을 반대 순서로 호출)

#### 기울기 검증

수치 미분의 결과와 오차역전파법의 결과를 비교하여 오차역전파법을 제대로 구현했는지 검증

```python
import sys, os
sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정
import numpy as np
from dataset.mnist import load_mnist
from two_layer_net import TwoLayerNet

# 데이터 읽기
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

x_batch = x_train[:3]
t_batch = t_train[:3]

grad_numerical = network.numerical_gradient(x_batch, t_batch)
grad_backprop = network.gradient(x_batch, t_batch)

# 각 가중치의 절대 오차의 평균을 구한다.
for key in grad_numerical.keys():
    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )
    print(key + ":" + str(diff))

# 출력
# W1:8.354615773332727e-08
# b1:9.44487058925185e-07
# W2:6.084831780796747e-09
# b2:1.3933655495929064e-07
```

위의 결과는 **수치 미분과 오차역전파법으로 구한 기울기의 차이가 매우 작다는 것을 보여줌**

**(오차역전파법으로 구한 기울기도 올바름)**

